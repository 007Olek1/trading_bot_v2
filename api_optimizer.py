#!/usr/bin/env python3
"""
‚ö° API OPTIMIZER & RATE LIMITER
==============================

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –±–∏—Ä–∂–µ:
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- –£–º–Ω—ã–π rate limiting
- Batch –∑–∞–ø—Ä–æ—Å—ã
- WebSocket –¥–ª—è real-time –¥–∞–Ω–Ω—ã—Ö
- –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
"""

import asyncio
import time
import hashlib
import json
from typing import Dict, List, Optional, Any
from collections import deque
from datetime import datetime, timedelta
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

class RateLimiter:
    """–£–º–Ω—ã–π rate limiter —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π"""
    
    def __init__(self, max_requests: int = 120, time_window: int = 60):
        """
        max_requests: –ú–∞–∫—Å–∏–º—É–º –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞ –æ–∫–Ω–æ –≤—Ä–µ–º–µ–Ω–∏
        time_window: –û–∫–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (Bybit: 120 req/min)
        """
        self.max_requests = max_requests
        self.time_window = time_window
        self.request_times = deque()
        self.last_rate_limit_hit = None
        self.adaptive_delay = 0.1  # –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
        self.consecutive_errors = 0
        
    async def acquire(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –∑–∞–ø—Ä–æ—Å"""
        now = time.time()
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã –≤–Ω–µ –æ–∫–Ω–∞
        while self.request_times and self.request_times[0] < now - self.time_window:
            self.request_times.popleft()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
        if len(self.request_times) >= self.max_requests:
            # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —Å–ª–æ—Ç–∞
            oldest_request = self.request_times[0]
            wait_time = self.time_window - (now - oldest_request) + 0.1
            
            if wait_time > 0:
                logger.debug(f"‚è≥ Rate limit: –æ–∂–∏–¥–∞–Ω–∏–µ {wait_time:.2f}—Å")
                await asyncio.sleep(wait_time)
                now = time.time()
                # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –ø–æ—Å–ª–µ –æ–∂–∏–¥–∞–Ω–∏—è
                while self.request_times and self.request_times[0] < now - self.time_window:
                    self.request_times.popleft()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å
        self.request_times.append(now)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É
        if self.adaptive_delay > 0:
            await asyncio.sleep(self.adaptive_delay)
    
    def on_rate_limit_error(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ rate limit"""
        self.consecutive_errors += 1
        self.adaptive_delay = min(self.adaptive_delay * 1.5, 1.0)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
        self.last_rate_limit_hit = time.time()
        logger.warning(f"‚ö†Ô∏è Rate limit –¥–æ—Å—Ç–∏–≥–Ω—É—Ç, –∑–∞–¥–µ—Ä–∂–∫–∞ —É–≤–µ–ª–∏—á–µ–Ω–∞ –¥–æ {self.adaptive_delay:.2f}—Å")
    
    def on_success(self):
        """–ü—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ —É–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É"""
        if self.consecutive_errors > 0:
            self.consecutive_errors -= 1
            if self.consecutive_errors == 0:
                self.adaptive_delay = max(self.adaptive_delay * 0.9, 0.05)  # –£–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É

class DataCache:
    """–£–º–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, cache_dir: str = "data/cache", default_ttl: int = 30):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.default_ttl = default_ttl  # TTL –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        self.memory_cache = {}  # –ë—ã—Å—Ç—Ä—ã–π in-memory –∫—ç—à
        
    def _get_cache_key(self, endpoint: str, params: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        key_data = f"{endpoint}:{json.dumps(params, sort_keys=True)}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get(self, endpoint: str, params: Dict) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞"""
        cache_key = self._get_cache_key(endpoint, params)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
        if cache_key in self.memory_cache:
            data, timestamp, ttl = self.memory_cache[cache_key]
            if time.time() - timestamp < ttl:
                return data
            else:
                del self.memory_cache[cache_key]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª–æ–≤—ã–π –∫—ç—à
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r') as f:
                    cache_data = json.load(f)
                    if time.time() - cache_data['timestamp'] < cache_data['ttl']:
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø–∞–º—è—Ç—å
                        self.memory_cache[cache_key] = (
                            cache_data['data'],
                            cache_data['timestamp'],
                            cache_data['ttl']
                        )
                        return cache_data['data']
            except Exception as e:
                logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞: {e}")
        
        return None
    
    def set(self, endpoint: str, params: Dict, data: Dict, ttl: Optional[int] = None):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à"""
        cache_key = self._get_cache_key(endpoint, params)
        ttl = ttl or self.default_ttl
        timestamp = time.time()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
        self.memory_cache[cache_key] = (data, timestamp, ttl)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            with open(cache_file, 'w') as f:
                json.dump({
                    'data': data,
                    'timestamp': timestamp,
                    'ttl': ttl
                }, f)
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –∫—ç—à–∞: {e}")
    
    def clear_old_cache(self, max_age_hours: int = 24):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä–æ–≥–æ –∫—ç—à–∞"""
        cutoff_time = time.time() - (max_age_hours * 3600)
        cleared = 0
        
        for cache_file in self.cache_dir.glob("*.json"):
            try:
                if cache_file.stat().st_mtime < cutoff_time:
                    cache_file.unlink()
                    cleared += 1
            except Exception:
                pass
        
        if cleared > 0:
            logger.info(f"üßπ –û—á–∏—â–µ–Ω–æ {cleared} —Å—Ç–∞—Ä—ã—Ö –∫—ç—à-—Ñ–∞–π–ª–æ–≤")

class RequestBatcher:
    """Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self, batch_size: int = 10, batch_timeout: float = 0.5):
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.pending_requests = []
        self.last_batch_time = time.time()
        
    async def add_request(self, request_func, *args, **kwargs):
        """–î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –≤ batch"""
        future = asyncio.Future()
        self.pending_requests.append((future, request_func, args, kwargs))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç—å batch
        should_execute = (
            len(self.pending_requests) >= self.batch_size or
            (time.time() - self.last_batch_time) >= self.batch_timeout
        )
        
        if should_execute:
            await self._execute_batch()
        
        return await future
    
    async def _execute_batch(self):
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å batch –∑–∞–ø—Ä–æ—Å–æ–≤"""
        if not self.pending_requests:
            return
        
        requests_to_process = self.pending_requests[:self.batch_size]
        self.pending_requests = self.pending_requests[self.batch_size:]
        self.last_batch_time = time.time()
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = []
        for future, request_func, args, kwargs in requests_to_process:
            task = asyncio.create_task(self._execute_single(future, request_func, args, kwargs))
            tasks.append(task)
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _execute_single(self, future, request_func, args, kwargs):
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å"""
        try:
            result = await request_func(*args, **kwargs)
            if not future.cancelled():
                future.set_result(result)
        except Exception as e:
            if not future.cancelled():
                future.set_exception(e)

class APIOptimizer:
    """‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä API –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self, exchange, cache_dir: str = "data/cache"):
        self.exchange = exchange
        self.rate_limiter = RateLimiter(max_requests=100, time_window=60)  # 100 req/min –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        self.cache = DataCache(cache_dir=cache_dir)
        self.batcher = RequestBatcher(batch_size=5, batch_timeout=0.3)
        self.request_stats = {
            'total': 0,
            'cached': 0,
            'batched': 0,
            'errors': 0
        }
        
    def _normalize_symbol(self, symbol: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–∏–º–≤–æ–ª, —É–±–∏—Ä–∞—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ USDT"""
        if not symbol:
            return symbol
        
        norm = symbol.upper().replace('/', '').replace('-', '')
        # –£–±–∏—Ä–∞–µ–º :USDT –µ—Å–ª–∏ –µ—Å—Ç—å
        if norm.endswith(':USDT'):
            norm = norm[:-5] + 'USDT'
        elif ':USDT' in norm:
            norm = norm.replace(':USDT', '') + 'USDT'
        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ ':'
        norm = norm.replace(':', '')
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ USDT
        if not norm.endswith('USDT'):
            norm = norm + 'USDT'
        # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω–æ–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ USDT –≤ –∫–æ–Ω—Ü–µ
        while norm.endswith('USDTUSDT'):
            norm = norm[:-4]
        return norm
        
    async def fetch_with_cache(self, method: str, symbol: str, 
                              timeframe: str = '15m', 
                              limit: int = 100,
                              cache_ttl: int = 30) -> Optional[List]:
        """
        –ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ rate limiting
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–∏–º–≤–æ–ª –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
        normalized_symbol = self._normalize_symbol(symbol)
        
        params = {
            'symbol': normalized_symbol,
            'timeframe': timeframe,
            'limit': limit
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_data = self.cache.get(method, params)
        if cached_data:
            self.request_stats['cached'] += 1
            logger.debug(f"‚úÖ –ö—ç—à hit: {normalized_symbol} {timeframe}")
            return cached_data
        
        # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å —Å rate limiting
        try:
            await self.rate_limiter.acquire()
            self.request_stats['total'] += 1
            
            if method == 'fetch_ohlcv':
                data = await self.exchange.fetch_ohlcv(normalized_symbol, timeframe, limit=limit)
            elif method == 'fetch_ticker':
                data = await self.exchange.fetch_ticker(normalized_symbol)
            else:
                data = None
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            if data:
                self.cache.set(method, params, data, cache_ttl)
                self.rate_limiter.on_success()
                return data
            
        except Exception as e:
            self.request_stats['errors'] += 1
            error_str = str(e).lower()
            
            if 'rate limit' in error_str or '429' in error_str:
                self.rate_limiter.on_rate_limit_error()
                logger.warning(f"‚ö†Ô∏è Rate limit error –¥–ª—è {normalized_symbol}")
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {normalized_symbol}: {e}")
            
            # –ü—Ä–æ–±—É–µ–º –≤–µ—Ä–Ω—É—Ç—å –∏–∑ –∫—ç—à–∞ –¥–∞–∂–µ –µ—Å–ª–∏ —Å—Ç–∞—Ä—ã–π
            cached_data = self.cache.get(method, params)
            if cached_data:
                logger.debug(f"üíæ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –∫—ç—à –¥–ª—è {normalized_symbol}")
                return cached_data
            
            return None
        
        return None
    
    async def fetch_multiple_symbols(self, symbols: List[str], 
                                     timeframe: str = '15m',
                                     limit: int = 100,
                                     max_concurrent: int = 5) -> Dict[str, Optional[List]]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º"""
        results = {}
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
        for i in range(0, len(symbols), max_concurrent):
            batch = symbols[i:i + max_concurrent]
            
            tasks = [
                self.fetch_with_cache('fetch_ohlcv', symbol, timeframe, limit)
                for symbol in batch
            ]
            
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for symbol, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–ª—è {symbol}: {result}")
                    results[symbol] = None
                else:
                    results[symbol] = result
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            if i + max_concurrent < len(symbols):
                await asyncio.sleep(0.1)
        
        return results
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        cache_hit_rate = (
            (self.request_stats['cached'] / self.request_stats['total'] * 100)
            if self.request_stats['total'] > 0 else 0
        )
        
        return {
            'total_requests': self.request_stats['total'],
            'cached_requests': self.request_stats['cached'],
            'cache_hit_rate': f"{cache_hit_rate:.1f}%",
            'batched_requests': self.request_stats['batched'],
            'errors': self.request_stats['errors'],
            'rate_limiter_delay': self.rate_limiter.adaptive_delay
        }
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à"""
        self.cache.clear_old_cache(max_age_hours=1)
        self.cache.memory_cache.clear()


