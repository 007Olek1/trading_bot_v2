#!/usr/bin/env python3
"""
ü§ñ –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –°–ê–ú–û–û–ë–£–ß–ê–Æ–©–ò–•–°–Ø –ê–ì–ï–ù–¢–û–í
==================================================

–°–∏—Å—Ç–µ–º–∞ –∞–≥–µ–Ω—Ç–æ–≤ —Å:
- –°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ (–Ω–µ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ)
- –≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º (–º—É—Ç–∞—Ü–∏–∏, –∫—Ä–æ—Å—Å–æ–≤–µ—Ä)
- –û–±–º–µ–Ω–æ–º –∑–Ω–∞–Ω–∏—è–º–∏ –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–∞–º–∏
- –ú–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ–º (–æ–±—É—á–µ–Ω–∏–µ —É—á–∏—Ç—å—Å—è)
- –ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º
"""

import asyncio
import json
import random
import statistics
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict, field
from collections import defaultdict, deque
import logging
from pathlib import Path
import numpy as np

logger = logging.getLogger(__name__)

@dataclass
class AgentKnowledge:
    """–ó–Ω–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞, –Ω–µ —Ç–æ—á–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è)"""
    agent_id: str
    agent_type: str  # 'cleaner', 'security', 'stability', 'recovery'
    learned_rules: Dict[str, Dict] = field(default_factory=dict)  # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
    adaptation_strategies: Dict[str, float] = field(default_factory=dict)  # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
    performance_history: List[Dict] = field(default_factory=list)
    success_rate: float = 0.0
    generalization_score: float = 0.0
    last_updated: str = ""
    evolution_generation: int = 0

@dataclass
class LearningPattern:
    """–ü–∞—Ç—Ç–µ—Ä–Ω –æ–±—É—á–µ–Ω–∏—è (–¥–∏–∞–ø–∞–∑–æ–Ω—ã —É—Å–ª–æ–≤–∏–π)"""
    pattern_id: str
    condition_ranges: Dict[str, Tuple[float, float]]  # –î–∏–∞–ø–∞–∑–æ–Ω—ã, –Ω–µ —Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    action: str
    success_count: int
    failure_count: int
    market_conditions: List[str]
    generalization_level: float
    created_from_examples: int

class EvolutionaryLearning:
    """üß¨ –≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.mutation_rate = 0.15
        self.crossover_rate = 0.25
        self.selection_pressure = 0.3
        self.generation = 0
        
    def create_rule_from_patterns(self, patterns: List[Dict], min_samples: int = 5) -> Dict:
        """–°–æ–∑–¥–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ –∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (–¥–∏–∞–ø–∞–∑–æ–Ω—ã)"""
        if len(patterns) < min_samples:
            return None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        feature_ranges = {}
        for feature in patterns[0].get('features', {}).keys():
            values = []
            for pattern in patterns:
                val = pattern.get('features', {}).get(feature)
                if val is not None:
                    try:
                        values.append(float(val))
                    except (ValueError, TypeError):
                        continue
            
            if len(values) >= 3:
                # –°–æ–∑–¥–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω —Å –∑–∞–ø–∞—Å–æ–º (–Ω–µ —Ç–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ!)
                min_val = min(values)
                max_val = max(values)
                std_val = statistics.stdev(values) if len(values) > 1 else 0
                
                # –†–∞—Å—à–∏—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è
                range_min = min_val - (std_val * 0.5)
                range_max = max_val + (std_val * 0.5)
                
                feature_ranges[feature] = (range_min, range_max)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
        successful = sum(1 for p in patterns if p.get('result') == 'success')
        success_rate = successful / len(patterns)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
        generalization = self._calculate_generalization(patterns, feature_ranges)
        
        return {
            'feature_ranges': feature_ranges,
            'success_rate': success_rate,
            'generalization_score': generalization,
            'sample_size': len(patterns),
            'market_conditions': list(set([p.get('market_condition', 'NEUTRAL') for p in patterns]))
        }
    
    def _calculate_generalization(self, patterns: List[Dict], ranges: Dict) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å –æ–±–æ–±—â–µ–Ω–∏—è (0-1)"""
        if not ranges:
            return 0.0
        
        # –®–∏—Ä–∏–Ω–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        range_widths = []
        for feature, (min_val, max_val) in ranges.items():
            if abs(min_val) > 0.0001:
                width = (max_val - min_val) / abs(min_val)
                range_widths.append(width)
        
        if not range_widths:
            return 0.0
        
        # –ß–µ–º —à–∏—Ä–µ –¥–∏–∞–ø–∞–∑–æ–Ω (–Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º), —Ç–µ–º –ª—É—á—à–µ –æ–±–æ–±—â–µ–Ω–∏–µ
        avg_width = statistics.mean(range_widths)
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º (–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —à–∏—Ä–∏–Ω–∞ ~0.3-0.5)
        generalization = min(avg_width / 0.5, 1.0) if avg_width > 0 else 0.0
        
        return generalization
    
    def mutate_rule(self, rule: Dict, mutation_strength: float = 0.1) -> Dict:
        """–ú—É—Ç–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —ç–≤–æ–ª—é—Ü–∏–∏"""
        mutated_rule = rule.copy()
        
        if random.random() < self.mutation_rate:
            # –ú—É—Ç–∏—Ä—É–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã
            if 'feature_ranges' in mutated_rule:
                for feature, (min_val, max_val) in mutated_rule['feature_ranges'].items():
                    if random.random() < 0.3:  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º—É—Ç–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞
                        range_width = max_val - min_val
                        mutation = range_width * mutation_strength * random.uniform(-1, 1)
                        
                        mutated_rule['feature_ranges'][feature] = (
                            min_val + mutation,
                            max_val + mutation
                        )
            
            # –ú—É—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            if 'strategy_params' in mutated_rule:
                for param, value in mutated_rule['strategy_params'].items():
                    if isinstance(value, (int, float)):
                        mutation = value * mutation_strength * random.uniform(-0.2, 0.2)
                        mutated_rule['strategy_params'][param] = value + mutation
        
        return mutated_rule
    
    def crossover_rules(self, rule1: Dict, rule2: Dict) -> Dict:
        """–°–∫—Ä–µ—â–∏–≤–∞–Ω–∏–µ –¥–≤—É—Ö –ø—Ä–∞–≤–∏–ª"""
        if random.random() > self.crossover_rate:
            return rule1  # –ù–µ—Ç —Å–∫—Ä–µ—â–∏–≤–∞–Ω–∏—è
        
        offspring = rule1.copy()
        
        # –°–∫—Ä–µ—â–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if 'feature_ranges' in rule1 and 'feature_ranges' in rule2:
            for feature in set(list(rule1['feature_ranges'].keys()) + list(rule2['feature_ranges'].keys())):
                if feature in rule1['feature_ranges'] and feature in rule2['feature_ranges']:
                    # –ë–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
                    min1, max1 = rule1['feature_ranges'][feature]
                    min2, max2 = rule2['feature_ranges'][feature]
                    
                    offspring['feature_ranges'][feature] = (
                        (min1 + min2) / 2,
                        (max1 + max2) / 2
                    )
        
        return offspring
    
    def select_best_rules(self, rules: List[Dict], top_n: int = 10) -> List[Dict]:
        """–û—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –ø—Ä–∞–≤–∏–ª (—ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç–±–æ—Ä)"""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–µ–Ω–∏—è
        scored_rules = []
        for rule in rules:
            score = (
                rule.get('success_rate', 0) * 0.6 +
                rule.get('generalization_score', 0) * 0.4
            )
            scored_rules.append((score, rule))
        
        scored_rules.sort(reverse=True)
        
        # –û—Ç–±–∏—Ä–∞–µ–º —Ç–æ–ø-N
        return [rule for _, rule in scored_rules[:top_n]]

class KnowledgeSharing:
    """ü§ù –°–∏—Å—Ç–µ–º–∞ –æ–±–º–µ–Ω–∞ –∑–Ω–∞–Ω–∏—è–º–∏ –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–∞–º–∏"""
    
    def __init__(self, knowledge_dir: str = "data/knowledge"):
        self.knowledge_dir = Path(knowledge_dir)
        self.knowledge_dir.mkdir(parents=True, exist_ok=True)
        self.shared_knowledge = {}
        self.knowledge_updates = deque(maxlen=100)
        
    def save_agent_knowledge(self, agent_id: str, knowledge: AgentKnowledge):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–Ω–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–∞"""
        try:
            knowledge_file = self.knowledge_dir / f"{agent_id}_knowledge.json"
            
            knowledge_dict = {
                'agent_id': knowledge.agent_id,
                'agent_type': knowledge.agent_type,
                'learned_rules': knowledge.learned_rules,
                'adaptation_strategies': knowledge.adaptation_strategies,
                'success_rate': knowledge.success_rate,
                'generalization_score': knowledge.generalization_score,
                'last_updated': datetime.now().isoformat(),
                'evolution_generation': knowledge.evolution_generation
            }
            
            with open(knowledge_file, 'w') as f:
                json.dump(knowledge_dict, f, indent=2)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–µ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            self.shared_knowledge[agent_id] = knowledge_dict
            self.knowledge_updates.append({
                'agent_id': agent_id,
                'timestamp': datetime.now().isoformat(),
                'update_type': 'knowledge_saved'
            })
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞ {agent_id}: {e}")
    
    def load_agent_knowledge(self, agent_id: str) -> Optional[AgentKnowledge]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∑–Ω–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–∞"""
        try:
            knowledge_file = self.knowledge_dir / f"{agent_id}_knowledge.json"
            
            if not knowledge_file.exists():
                return None
            
            with open(knowledge_file, 'r') as f:
                data = json.load(f)
            
            return AgentKnowledge(
                agent_id=data['agent_id'],
                agent_type=data['agent_type'],
                learned_rules=data.get('learned_rules', {}),
                adaptation_strategies=data.get('adaptation_strategies', {}),
                success_rate=data.get('success_rate', 0.0),
                generalization_score=data.get('generalization_score', 0.0),
                last_updated=data.get('last_updated', ''),
                evolution_generation=data.get('evolution_generation', 0)
            )
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞ {agent_id}: {e}")
            return None
    
    def share_knowledge(self, from_agent: str, to_agent: str, rule_name: str) -> bool:
        """–ü–æ–¥–µ–ª–∏—Ç—å—Å—è –ø—Ä–∞–≤–∏–ª–æ–º –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–∞–º–∏"""
        try:
            from_knowledge = self.load_agent_knowledge(from_agent)
            if not from_knowledge or rule_name not in from_knowledge.learned_rules:
                return False
            
            to_knowledge = self.load_agent_knowledge(to_agent)
            if not to_knowledge:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –∑–Ω–∞–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–∞—Ç–µ–ª—è
                to_knowledge = AgentKnowledge(
                    agent_id=to_agent,
                    agent_type=from_knowledge.agent_type
                )
            
            # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª–æ –ø–æ–¥ —Ç–∏–ø –∞–≥–µ–Ω—Ç–∞ –ø–æ–ª—É—á–∞—Ç–µ–ª—è
            rule = from_knowledge.learned_rules[rule_name].copy()
            rule['shared_from'] = from_agent
            rule['adaptation_date'] = datetime.now().isoformat()
            
            to_knowledge.learned_rules[rule_name] = rule
            self.save_agent_knowledge(to_agent, to_knowledge)
            
            logger.info(f"ü§ù –ó–Ω–∞–Ω–∏–µ '{rule_name}' –ø–µ—Ä–µ–¥–∞–Ω–æ –æ—Ç {from_agent} –∫ {to_agent}")
            return True
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–º–µ–Ω–∞ –∑–Ω–∞–Ω–∏—è–º–∏: {e}")
            return False
    
    def find_similar_agents(self, agent_type: str) -> List[str]:
        """–ù–∞–π—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ —Ç–æ–≥–æ –∂–µ —Ç–∏–ø–∞ –¥–ª—è –æ–±–º–µ–Ω–∞ –æ–ø—ã—Ç–æ–º"""
        similar = []
        for agent_id, knowledge_data in self.shared_knowledge.items():
            if knowledge_data.get('agent_type') == agent_type:
                similar.append(agent_id)
        return similar
    
    def get_collective_wisdom(self, agent_type: str) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—É—é –º—É–¥—Ä–æ—Å—Ç—å –¥–ª—è —Ç–∏–ø–∞ –∞–≥–µ–Ω—Ç–æ–≤"""
        all_rules = defaultdict(list)
        
        for agent_id, knowledge_data in self.shared_knowledge.items():
            if knowledge_data.get('agent_type') == agent_type:
                for rule_name, rule_data in knowledge_data.get('learned_rules', {}).items():
                    all_rules[rule_name].append({
                        'source': agent_id,
                        'success_rate': rule_data.get('success_rate', 0),
                        'rule': rule_data
                    })
        
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–ø—ã—Ç–∞
        collective_rules = {}
        for rule_name, rule_instances in all_rules.items():
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
            rule_instances.sort(key=lambda x: x['success_rate'], reverse=True)
            # –ë–µ—Ä–µ–º –ª—É—á—à–µ–µ –ø—Ä–∞–≤–∏–ª–æ
            if rule_instances:
                collective_rules[rule_name] = rule_instances[0]['rule']
        
        return collective_rules

class MetaLearningSystem:
    """üß† –ú–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ: –æ–±—É—á–µ–Ω–∏–µ —É—á–∏—Ç—å—Å—è"""
    
    def __init__(self):
        self.learning_methods = {}
        self.method_performance = defaultdict(list)
        self.optimal_methods = {}
        
    def learn_optimal_learning_method(self, task_type: str, 
                                      method_params: Dict,
                                      performance: float):
        """–ò–∑—É—á–∏—Ç—å –∫–∞–∫–æ–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ª—É—á—à–µ –¥–ª—è –∑–∞–¥–∞—á–∏"""
        if task_type not in self.learning_methods:
            self.learning_methods[task_type] = []
        
        self.learning_methods[task_type].append({
            'params': method_params,
            'performance': performance,
            'timestamp': datetime.now().isoformat()
        })
        
        self.method_performance[task_type].append(performance)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥
        if len(self.method_performance[task_type]) > 10:
            avg_performance = statistics.mean(self.method_performance[task_type][-10:])
            if task_type not in self.optimal_methods or \
               avg_performance > self.optimal_methods[task_type].get('performance', 0):
                # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ–ø—ã—Ç—ã
                recent_methods = self.learning_methods[task_type][-10:]
                best_method = max(recent_methods, key=lambda x: x['performance'])
                
                self.optimal_methods[task_type] = {
                    'params': best_method['params'],
                    'performance': avg_performance,
                    'last_updated': datetime.now().isoformat()
                }
        
        logger.debug(f"üß† –ú–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ: –æ–±–Ω–æ–≤–ª–µ–Ω –º–µ—Ç–æ–¥ –¥–ª—è {task_type}")
    
    def get_optimal_method(self, task_type: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞—á–∏"""
        if task_type not in self.optimal_methods:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            return {
                'mutation_rate': 0.15,
                'crossover_rate': 0.25,
                'selection_pressure': 0.3,
                'min_samples': 5
            }
        
        return self.optimal_methods[task_type].get('params')

class IntelligentAgent:
    """ü§ñ –ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º"""
    
    def __init__(self, agent_id: str, agent_type: str):
        self.agent_id = agent_id
        self.agent_type = agent_type
        self.evolutionary_learning = EvolutionaryLearning()
        self.knowledge_sharing = KnowledgeSharing()
        self.meta_learning = MetaLearningSystem()
        
        self.knowledge = self.knowledge_sharing.load_agent_knowledge(agent_id)
        if not self.knowledge:
            self.knowledge = AgentKnowledge(
                agent_id=agent_id,
                agent_type=agent_type,
                last_updated=datetime.now().isoformat()
            )
        
        self.experience_history = deque(maxlen=1000)
        self.performance_metrics = deque(maxlen=100)
        
    def learn_from_experience(self, experience: Dict):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–ø—ã—Ç–µ (—Å–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª)"""
        self.experience_history.append({
            **experience,
            'timestamp': datetime.now().isoformat()
        })
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –æ–ø—ã—Ç –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
        successful_experiences = [
            e for e in self.experience_history
            if e.get('result') == 'success'
        ]
        
        failed_experiences = [
            e for e in self.experience_history
            if e.get('result') == 'failure'
        ]
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–ø—ã—Ç–∞ (–º–∏–Ω–∏–º—É–º 5 –ø—Ä–∏–º–µ—Ä–æ–≤)
        if len(successful_experiences) >= 5:
            rule = self.evolutionary_learning.create_rule_from_patterns(
                successful_experiences,
                min_samples=5
            )
            
            if rule and rule.get('generalization_score', 0) > 0.5:
                rule_name = f"{self.agent_type}_rule_{len(self.knowledge.learned_rules)}"
                rule['created_at'] = datetime.now().isoformat()
                rule['agent_id'] = self.agent_id
                
                self.knowledge.learned_rules[rule_name] = rule
                self.knowledge.success_rate = rule['success_rate']
                self.knowledge.generalization_score = rule['generalization_score']
                self.knowledge.last_updated = datetime.now().isoformat()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞–Ω–∏—è
                self.knowledge_sharing.save_agent_knowledge(self.agent_id, self.knowledge)
                
                logger.info(f"üß† {self.agent_id} —Å–æ–∑–¥–∞–ª —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ: {rule_name} "
                          f"(—É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {rule['success_rate']:.1%}, –æ–±–æ–±—â–µ–Ω–∏–µ: {rule['generalization_score']:.2f})")
        
        # –≠–≤–æ–ª—é—Ü–∏—è –ø—Ä–∞–≤–∏–ª (–∫–∞–∂–¥—ã–µ 20 –æ–ø—ã—Ç–æ–≤)
        if len(self.experience_history) % 20 == 0 and len(self.knowledge.learned_rules) > 1:
            self._evolve_rules()
    
    def _evolve_rules(self):
        """–≠–≤–æ–ª—é—Ü–∏—è –ø—Ä–∞–≤–∏–ª (–º—É—Ç–∞—Ü–∏–∏ –∏ —Å–∫—Ä–µ—â–∏–≤–∞–Ω–∏–µ)"""
        rules = list(self.knowledge.learned_rules.values())
        
        if len(rules) < 2:
            return
        
        # –û—Ç–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ –ø—Ä–∞–≤–∏–ª–∞
        best_rules = self.evolutionary_learning.select_best_rules(rules, top_n=5)
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —á–µ—Ä–µ–∑ –º—É—Ç–∞—Ü–∏–∏ –∏ —Å–∫—Ä–µ—â–∏–≤–∞–Ω–∏–µ
        new_rules = []
        for i, rule in enumerate(best_rules):
            # –ú—É—Ç–∞—Ü–∏—è
            mutated = self.evolutionary_learning.mutate_rule(rule.copy())
            new_rules.append(mutated)
            
            # –°–∫—Ä–µ—â–∏–≤–∞–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º –ø—Ä–∞–≤–∏–ª–æ–º
            if i < len(best_rules) - 1:
                crossover = self.evolutionary_learning.crossover_rules(
                    rule, best_rules[i + 1]
                )
                new_rules.append(crossover)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∏ –∑–∞–º–µ–Ω—è–µ–º —Ö—É–¥—à–∏–µ
        for new_rule in new_rules[:3]:  # –ë–µ—Ä–µ–º —Ç–æ–ø-3 –Ω–æ–≤—ã—Ö
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            applicable_experiences = [
                e for e in self.experience_history
                if self._rule_matches_experience(new_rule, e)
            ]
            
            if applicable_experiences:
                success_count = sum(1 for e in applicable_experiences if e.get('result') == 'success')
                new_rule['success_rate'] = success_count / len(applicable_experiences)
                
                # –ï—Å–ª–∏ –Ω–æ–≤–æ–µ –ø—Ä–∞–≤–∏–ª–æ –ª—É—á—à–µ —Å—Ç–∞—Ä–æ–≥–æ, –∑–∞–º–µ–Ω—è–µ–º
                worst_rule_name = min(
                    self.knowledge.learned_rules.keys(),
                    key=lambda k: self.knowledge.learned_rules[k].get('success_rate', 0)
                )
                
                if new_rule['success_rate'] > self.knowledge.learned_rules[worst_rule_name].get('success_rate', 0):
                    new_rule_name = f"{self.agent_type}_evolved_{self.knowledge.evolution_generation}"
                    new_rule['evolution_generation'] = self.knowledge.evolution_generation + 1
                    self.knowledge.learned_rules[new_rule_name] = new_rule
                    del self.knowledge.learned_rules[worst_rule_name]
        
        self.knowledge.evolution_generation += 1
        self.knowledge_sharing.save_agent_knowledge(self.agent_id, self.knowledge)
    
    def _rule_matches_experience(self, rule: Dict, experience: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –ø–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ –ø—Ä–∞–≤–∏–ª–æ –∫ –æ–ø—ã—Ç—É"""
        if 'feature_ranges' not in rule:
            return False
        
        features = experience.get('features', {})
        for feature, (min_val, max_val) in rule['feature_ranges'].items():
            if feature in features:
                value = float(features[feature])
                if not (min_val <= value <= max_val):
                    return False
        
        return True
    
    def apply_learned_rules(self, context: Dict) -> Optional[Dict]:
        """–ü—Ä–∏–º–µ–Ω–∏—Ç—å –∏–∑—É—á–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
        best_rule = None
        best_score = 0.0
        
        for rule_name, rule in self.knowledge.learned_rules.items():
            if self._rule_matches_experience(rule, context):
                score = (
                    rule.get('success_rate', 0) * 0.7 +
                    rule.get('generalization_score', 0) * 0.3
                )
                if score > best_score:
                    best_score = score
                    best_rule = rule
        
        return best_rule
    
    def share_knowledge_with_peers(self):
        """–ü–æ–¥–µ–ª–∏—Ç—å—Å—è –∑–Ω–∞–Ω–∏—è–º–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏"""
        similar_agents = self.knowledge_sharing.find_similar_agents(self.agent_type)
        
        for peer_id in similar_agents:
            if peer_id != self.agent_id:
                # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–µ–µ –ø—Ä–∞–≤–∏–ª–æ –¥–ª—è –æ–±–º–µ–Ω–∞
                best_rule_name = max(
                    self.knowledge.learned_rules.keys(),
                    key=lambda k: self.knowledge.learned_rules[k].get('success_rate', 0),
                    default=None
                )
                
                if best_rule_name:
                    self.knowledge_sharing.share_knowledge(
                        self.agent_id, peer_id, best_rule_name
                    )
    
    def learn_from_collective_wisdom(self):
        """–ò–∑—É—á–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—É—é –º—É–¥—Ä–æ—Å—Ç—å –¥—Ä—É–≥–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤"""
        collective_rules = self.knowledge_sharing.get_collective_wisdom(self.agent_type)
        
        for rule_name, rule in collective_rules.items():
            # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ –ø–æ–¥ —Å–µ–±—è
            if rule_name not in self.knowledge.learned_rules:
                rule_copy = rule.copy()
                rule_copy['adapted_from_collective'] = True
                rule_copy['adaptation_date'] = datetime.now().isoformat()
                self.knowledge.learned_rules[rule_name] = rule_copy
        
        self.knowledge_sharing.save_agent_knowledge(self.agent_id, self.knowledge)

class IntelligentAgentsSystem:
    """üåê –°–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.agents = {}
        self.knowledge_sharing = KnowledgeSharing()
        self.meta_learning = MetaLearningSystem()
        
    def create_agent(self, agent_id: str, agent_type: str) -> IntelligentAgent:
        """–°–æ–∑–¥–∞—Ç—å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞"""
        agent = IntelligentAgent(agent_id, agent_type)
        self.agents[agent_id] = agent
        return agent
    
    async def run_learning_cycle(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤"""
        logger.info("üß† –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤")
        
        tasks = []
        for agent_id, agent in self.agents.items():
            # –ê–≥–µ–Ω—Ç—ã —É—á–∞—Ç—Å—è –¥—Ä—É–≥ —É –¥—Ä—É–≥–∞
            tasks.append(asyncio.create_task(self._agent_learning_process(agent)))
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _agent_learning_process(self, agent: IntelligentAgent):
        """–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞"""
        try:
            # 1. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–ø—ã—Ç–µ
            if len(agent.experience_history) >= 5:
                # –ê–≥–µ–Ω—Ç —É–∂–µ —Å–æ–∑–¥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ –æ–ø—ã—Ç–∞ –≤ learn_from_experience
                pass
            
            # 2. –≠–≤–æ–ª—é—Ü–∏—è –ø—Ä–∞–≤–∏–ª
            if len(agent.knowledge.learned_rules) > 1:
                agent._evolve_rules()
            
            # 3. –û–±–º–µ–Ω –∑–Ω–∞–Ω–∏—è–º–∏
            agent.share_knowledge_with_peers()
            
            # 4. –ò–∑—É—á–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–π –º—É–¥—Ä–æ—Å—Ç–∏
            agent.learn_from_collective_wisdom()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ {agent.agent_id}: {e}")
    
    def get_agent_statistics(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞–≥–µ–Ω—Ç–æ–≤"""
        stats = {}
        for agent_id, agent in self.agents.items():
            stats[agent_id] = {
                'type': agent.agent_type,
                'rules_count': len(agent.knowledge.learned_rules),
                'success_rate': agent.knowledge.success_rate,
                'generalization_score': agent.knowledge.generalization_score,
                'experience_count': len(agent.experience_history),
                'evolution_generation': agent.knowledge.evolution_generation
            }
        return stats

