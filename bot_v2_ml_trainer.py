#!/usr/bin/env python3
"""
üéì ML Model Trainer
–û–±—É—á–∞–µ—Ç DistilBERT –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –†–û–°–¢/–ü–ê–î–ï–ù–ò–ï/–ë–û–ö–û–í–ò–ö
"""

import asyncio
import logging
from typing import List, Dict, Tuple
import json
from datetime import datetime, timedelta
import numpy as np

logger = logging.getLogger(__name__)

try:
    from transformers import (
        DistilBertTokenizer,
        DistilBertForSequenceClassification,
        Trainer,
        TrainingArguments
    )
    from datasets import Dataset
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("‚ö†Ô∏è transformers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")


class MarketDatasetBuilder:
    """
    –°—Ç—Ä–æ–∏—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(self):
        self.data = []
    
    async def build_from_history(
        self,
        exchange_manager,
        symbols: List[str],
        days: int = 30
    ) -> List[Dict]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ–∑–¥–∞—ë—Ç –¥–∞—Ç–∞—Å–µ—Ç
        
        –§–æ—Ä–º–∞—Ç:
        {
            'text': "price rising strongly, volume increasing...",
            'label': 0/1/2  # 0=–ü–ê–î–ï–ù–ò–ï, 1=–ë–û–ö–û–í–ò–ö, 2=–†–û–°–¢
        }
        """
        
        logger.info(f"üìä –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ {len(symbols)} —Å–∏–º–≤–æ–ª–∞–º –∑–∞ {days} –¥–Ω–µ–π...")
        
        from bot_v2_nlp_analyzer import nlp_analyzer
        
        for symbol in symbols:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–≤–µ—á–∏
                candles = await exchange_manager.fetch_ohlcv(
                    symbol,
                    timeframe='1h',
                    limit=days * 24
                )
                
                if len(candles) < 50:
                    continue
                
                # –°–æ–∑–¥–∞—ë–º –æ–∫–Ω–∞ –¥–∞–Ω–Ω—ã—Ö
                window_size = 20
                for i in range(window_size, len(candles) - 5):
                    window = candles[i-window_size:i]
                    
                    # –ë—É–¥—É—â–µ–µ –¥–≤–∏–∂–µ–Ω–∏–µ (–º–µ—Ç–∫–∞)
                    current_price = candles[i]['close']
                    future_price = candles[i+5]['close']  # –ß–µ—Ä–µ–∑ 5 —á–∞—Å–æ–≤
                    
                    price_change = (future_price - current_price) / current_price * 100
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∫—É
                    if price_change > 2:
                        label = 2  # –†–û–°–¢
                    elif price_change < -2:
                        label = 0  # –ü–ê–î–ï–ù–ò–ï
                    else:
                        label = 1  # –ë–û–ö–û–í–ò–ö
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                    indicators = self._calculate_indicators(window)
                    description = nlp_analyzer.generate_market_description(
                        candles=window,
                        indicators=indicators,
                        current_price=current_price
                    )
                    
                    self.data.append({
                        'text': description,
                        'label': label,
                        'symbol': symbol,
                        'price_change': price_change
                    })
                
                logger.info(f"‚úÖ {symbol}: —Å–æ–±—Ä–∞–Ω–æ {len(self.data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö {symbol}: {e}")
                continue
        
        logger.info(f"üì¶ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(self.data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        return self.data
    
    def _calculate_indicators(self, candles: List[Dict]) -> Dict[str, float]:
        """–ë—ã—Å—Ç—Ä—ã–π —Ä–∞—Å—á—ë—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        import pandas as pd
        
        df = pd.DataFrame(candles)
        
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        
        # MACD
        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        macd = exp1 - exp2
        signal = macd.ewm(span=9, adjust=False).mean()
        
        return {
            'rsi': rsi.iloc[-1] if not pd.isna(rsi.iloc[-1]) else 50,
            'macd': macd.iloc[-1] if not pd.isna(macd.iloc[-1]) else 0,
            'macd_signal': signal.iloc[-1] if not pd.isna(signal.iloc[-1]) else 0
        }
    
    def save_dataset(self, filename: str = 'market_dataset.json'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–∞–π–ª"""
        with open(filename, 'w') as f:
            json.dump(self.data, f, indent=2)
        logger.info(f"üíæ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {filename}")
    
    def load_dataset(self, filename: str = 'market_dataset.json') -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ñ–∞–π–ª–∞"""
        with open(filename, 'r') as f:
            self.data = json.load(f)
        logger.info(f"üìÇ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        return self.data


class DistilBERTTrainer:
    """
    –û–±—É—á–∞–µ—Ç DistilBERT –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Ä—ã–Ω–∫–∞
    """
    
    def __init__(self):
        self.model_name = "distilbert-base-uncased"
        self.model = None
        self.tokenizer = None
        
        if not TRANSFORMERS_AVAILABLE:
            logger.error("‚ùå transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers datasets")
            return
        
        self.tokenizer = DistilBertTokenizer.from_pretrained(self.model_name)
        logger.info("‚úÖ Tokenizer –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    def prepare_dataset(self, data: List[Dict]) -> Dataset:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
        from collections import Counter
        label_counts = Counter([d['label'] for d in data])
        logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {label_counts}")
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        texts = [d['text'] for d in data]
        labels = [d['label'] for d in data]
        
        encodings = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=128
        )
        
        # –°–æ–∑–¥–∞—ë–º Dataset
        dataset_dict = {
            'input_ids': encodings['input_ids'],
            'attention_mask': encodings['attention_mask'],
            'labels': labels
        }
        
        dataset = Dataset.from_dict(dataset_dict)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
        split = dataset.train_test_split(test_size=0.2, seed=42)
        
        logger.info(f"üìö Train: {len(split['train'])}, Test: {len(split['test'])}")
        
        return split
    
    def train(
        self,
        dataset,
        output_dir: str = "./distilbert_market_classifier",
        epochs: int = 3,
        batch_size: int = 16
    ):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å"""
        
        if not TRANSFORMERS_AVAILABLE:
            logger.error("‚ùå transformers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        self.model = DistilBertForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=3  # –ü–ê–î–ï–ù–ò–ï, –ë–û–ö–û–í–ò–ö, –†–û–°–¢
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            warmup_steps=500,
            weight_decay=0.01,
            logging_dir='./logs',
            logging_steps=10,
            eval_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset['train'],
            eval_dataset=dataset['test'],
        )
        
        logger.info("üéì –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ DistilBERT...")
        
        # –û–±—É—á–µ–Ω–∏–µ
        trainer.train()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        trainer.save_model(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_dir}")
        
        # –û—Ü–µ–Ω–∫–∞
        results = trainer.evaluate()
        logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results}")
        
        return results
    
    def predict(self, text: str) -> Tuple[str, float]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Å –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        
        if not self.model:
            logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return "–ë–û–ö–û–í–ò–ö", 0.0
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=128
        )
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)
            predicted_class = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][predicted_class].item()
        
        # –ú–∞–ø–ø–∏–Ω–≥
        class_names = {0: "–ü–ê–î–ï–ù–ò–ï", 1: "–ë–û–ö–û–í–ò–ö", 2: "–†–û–°–¢"}
        
        return class_names[predicted_class], confidence


async def main():
    """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è"""
    
    if not TRANSFORMERS_AVAILABLE:
        print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:")
        print("pip install transformers datasets torch")
        return
    
    print("\n" + "="*60)
    print("üéì –û–ë–£–ß–ï–ù–ò–ï DISTILBERT –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –†–´–ù–ö–ê")
    print("="*60)
    
    # 1. –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("\nüìä –®–∞–≥ 1: –°–±–æ—Ä –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    from bot_v2_exchange import ExchangeManager
    
    exchange = ExchangeManager()
    await exchange.connect()
    
    builder = MarketDatasetBuilder()
    
    # –í—ã–±–∏—Ä–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–Ω–µ—Ç—ã
    symbols = [
        'BTC/USDT:USDT', 'ETH/USDT:USDT', 'SOL/USDT:USDT',
        'BNB/USDT:USDT', 'XRP/USDT:USDT', 'DOGE/USDT:USDT'
    ]
    
    data = await builder.build_from_history(exchange, symbols, days=7)
    builder.save_dataset()
    
    await exchange.disconnect()
    
    if len(data) < 100:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return
    
    # 2. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print("\nüéì –®–∞–≥ 2: –û–±—É—á–µ–Ω–∏–µ DistilBERT...")
    
    trainer = DistilBERTTrainer()
    dataset = trainer.prepare_dataset(data)
    
    results = trainer.train(
        dataset,
        epochs=3,
        batch_size=8
    )
    
    print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å: {results.get('eval_accuracy', 'N/A')}")
    
    # 3. –¢–µ—Å—Ç–∏—Ä—É–µ–º
    print("\nüß™ –®–∞–≥ 3: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...")
    
    test_texts = [
        "price rising strongly, volume surging, bullish momentum",
        "price falling sharply, volume declining, bearish momentum",
        "price stable, volume normal, sideways movement"
    ]
    
    for text in test_texts:
        prediction, confidence = trainer.predict(text)
        print(f"üìù '{text[:40]}...'")
        print(f"   ‚Üí {prediction} ({confidence:.0%})")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    asyncio.run(main())

