#!/usr/bin/env python3
"""
üß† –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –°–ò–°–¢–ï–ú–ê –ú–ê–®–ò–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø
==========================================

–§—É–Ω–∫—Ü–∏–∏:
- –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
- –ê–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
"""

import os
import numpy as np
import pandas as pd
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import json
import pickle
import joblib
from dataclasses import dataclass
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

@dataclass
class ModelPerformance:
    """üìä –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏"""
    model_name: str
    mse: float
    mae: float
    r2: float
    accuracy: float
    last_trained: datetime
    training_samples: int

@dataclass
class PredictionResult:
    """üîÆ –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    symbol: str
    predicted_price: float
    confidence: float
    trend_direction: str  # 'bullish', 'bearish', 'sideways'
    time_horizon: str    # 'short', 'medium', 'long'
    model_used: str
    features_importance: Dict[str, float]

class AdvancedMLSystem:
    """üß† –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self):
        # –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
        self.price_prediction_models = {}
        self.trend_prediction_models = {}
        self.strategy_optimization_models = {}
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        self.scalers = {}
        self.feature_importance = {}
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.settings = {
            'lookback_periods': [5, 10, 20, 50],  # –ü–µ—Ä–∏–æ–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            'prediction_horizons': [1, 3, 6, 12],  # –ì–æ—Ä–∏–∑–æ–Ω—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (—á–∞—Å—ã)
            'min_training_samples': 1000,
            'validation_split': 0.2,
            'retrain_frequency_hours': 24,
            'ensemble_weights': {
                'random_forest': 0.3,
                'gradient_boosting': 0.3,
                'neural_network': 0.2,
                'lstm': 0.2
            }
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_predictions': 0,
            'accurate_predictions': 0,
            'models_trained': 0,
            'last_training': None,
            'best_model': None,
            'avg_accuracy': 0.0
        }
        
        # –û—Ç–∫–ª—é—á–∞–µ–º CUDA –¥–ª—è TensorFlow
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
        
        logger.info("üß† –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ ML –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def create_features(self, ohlcv_data: pd.DataFrame, technical_indicators: Dict) -> pd.DataFrame:
        """üîß –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML"""
        features = pd.DataFrame()
        
        # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['price_change_1h'] = ohlcv_data['close'].pct_change(1)
        features['price_change_4h'] = ohlcv_data['close'].pct_change(4)
        features['price_change_24h'] = ohlcv_data['close'].pct_change(24)
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        features['volatility_1h'] = ohlcv_data['close'].rolling(4).std()
        features['volatility_4h'] = ohlcv_data['close'].rolling(16).std()
        features['volatility_24h'] = ohlcv_data['close'].rolling(96).std()
        
        # –û–±—ä—ë–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['volume_change_1h'] = ohlcv_data['volume'].pct_change(1)
        features['volume_ma_ratio'] = ohlcv_data['volume'] / ohlcv_data['volume'].rolling(20).mean()
        features['volume_spike'] = (ohlcv_data['volume'] > ohlcv_data['volume'].rolling(20).mean() * 2).astype(int)
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        if 'rsi' in technical_indicators:
            features['rsi'] = technical_indicators['rsi']
            features['rsi_overbought'] = (technical_indicators['rsi'] > 70).astype(int)
            features['rsi_oversold'] = (technical_indicators['rsi'] < 30).astype(int)
        
        if 'bb_upper' in technical_indicators and 'bb_lower' in technical_indicators:
            bb_position = (ohlcv_data['close'] - technical_indicators['bb_lower']) / (technical_indicators['bb_upper'] - technical_indicators['bb_lower'])
            features['bb_position'] = bb_position
            features['bb_squeeze'] = ((technical_indicators['bb_upper'] - technical_indicators['bb_lower']) / ohlcv_data['close'] < 0.1).astype(int)
        
        if 'ema_21' in technical_indicators and 'ema_50' in technical_indicators:
            features['ema_trend'] = (technical_indicators['ema_21'] > technical_indicators['ema_50']).astype(int)
            features['ema_distance'] = (ohlcv_data['close'] - technical_indicators['ema_21']) / technical_indicators['ema_21']
        
        # MACD
        if 'macd' in technical_indicators and 'macd_signal' in technical_indicators:
            macd_values = technical_indicators['macd']
            macd_signal_values = technical_indicators['macd_signal']
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ pandas Series –µ—Å–ª–∏ —ç—Ç–æ numpy array
            if isinstance(macd_values, np.ndarray):
                macd_series = pd.Series(macd_values, index=ohlcv_data.index)
                macd_signal_series = pd.Series(macd_signal_values, index=ohlcv_data.index)
            else:
                macd_series = macd_values
                macd_signal_series = macd_signal_values
            
            features['macd_diff'] = macd_series - macd_signal_series
            features['macd_cross'] = ((macd_series > macd_signal_series) & 
                                     (macd_series.shift(1) <= macd_signal_series.shift(1))).astype(int)
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['hour_of_day'] = pd.to_datetime(ohlcv_data.index).hour
        features['day_of_week'] = pd.to_datetime(ohlcv_data.index).dayofweek
        features['is_weekend'] = (features['day_of_week'] >= 5).astype(int)
        
        # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for lag in [1, 2, 3, 5, 10]:
            features[f'price_lag_{lag}'] = ohlcv_data['close'].shift(lag)
            features[f'volume_lag_{lag}'] = ohlcv_data['volume'].shift(lag)
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
        for period in [5, 10, 20, 50]:
            sma = ohlcv_data['close'].rolling(period).mean()
            features[f'sma_{period}'] = sma
            features[f'price_vs_sma_{period}'] = ohlcv_data['close'] / sma - 1
        
        # –£–¥–∞–ª—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        features = features.dropna()
        
        return features
    
    def create_targets(self, ohlcv_data: pd.DataFrame, prediction_horizons: List[int]) -> Dict[str, pd.Series]:
        """üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        targets = {}
        
        for horizon in prediction_horizons:
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω—ã
            targets[f'price_target_{horizon}h'] = ohlcv_data['close'].shift(-horizon)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
            price_change = ohlcv_data['close'].shift(-horizon) / ohlcv_data['close'] - 1
            targets[f'trend_target_{horizon}h'] = (price_change > 0.01).astype(int)  # 1% –ø–æ—Ä–æ–≥
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
            targets[f'volatility_target_{horizon}h'] = ohlcv_data['close'].rolling(horizon).std().shift(-horizon)
        
        return targets
    
    def train_price_prediction_model(self, symbol: str, features: pd.DataFrame, targets: Dict[str, pd.Series]) -> Dict[str, Any]:
        """üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω"""
        logger.info(f"üß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω –¥–ª—è {symbol}")
        
        models = {}
        performances = {}
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        X = features.values
        y_price = targets['price_target_1h'].values
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–æ–≤
        min_length = min(len(X), len(y_price))
        X = X[:min_length]
        y_price = y_price[:min_length]
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
        split_idx = int(len(X) * (1 - self.settings['validation_split']))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y_price[:split_idx], y_price[split_idx:]
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        self.scalers[f'{symbol}_price'] = scaler
        
        # 1. Random Forest
        rf_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        rf_model.fit(X_train_scaled, y_train)
        rf_pred = rf_model.predict(X_test_scaled)
        
        models['random_forest'] = rf_model
        performances['random_forest'] = ModelPerformance(
            model_name='Random Forest',
            mse=mean_squared_error(y_test, rf_pred),
            mae=mean_absolute_error(y_test, rf_pred),
            r2=r2_score(y_test, rf_pred),
            accuracy=self._calculate_direction_accuracy(y_test, rf_pred),
            last_trained=datetime.now(),
            training_samples=len(X_train)
        )
        
        # 2. Gradient Boosting
        gb_model = GradientBoostingRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=6,
            random_state=42
        )
        gb_model.fit(X_train_scaled, y_train)
        gb_pred = gb_model.predict(X_test_scaled)
        
        models['gradient_boosting'] = gb_model
        performances['gradient_boosting'] = ModelPerformance(
            model_name='Gradient Boosting',
            mse=mean_squared_error(y_test, gb_pred),
            mae=mean_absolute_error(y_test, gb_pred),
            r2=r2_score(y_test, gb_pred),
            accuracy=self._calculate_direction_accuracy(y_test, gb_pred),
            last_trained=datetime.now(),
            training_samples=len(X_train)
        )
        
        # 3. Neural Network
        nn_model = MLPRegressor(
            hidden_layer_sizes=(100, 50, 25),
            activation='relu',
            solver='adam',
            alpha=0.001,
            learning_rate='adaptive',
            max_iter=1000,
            random_state=42
        )
        nn_model.fit(X_train_scaled, y_train)
        nn_pred = nn_model.predict(X_test_scaled)
        
        models['neural_network'] = nn_model
        performances['neural_network'] = ModelPerformance(
            model_name='Neural Network',
            mse=mean_squared_error(y_test, nn_pred),
            mae=mean_absolute_error(y_test, nn_pred),
            r2=r2_score(y_test, nn_pred),
            accuracy=self._calculate_direction_accuracy(y_test, nn_pred),
            last_trained=datetime.now(),
            training_samples=len(X_train)
        )
        
        # 4. LSTM (–¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤)
        lstm_model = self._create_lstm_model(X_train_scaled.shape[1])
        lstm_model.fit(
            X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1]),
            y_train,
            epochs=50,
            batch_size=32,
            validation_split=0.2,
            callbacks=[
                EarlyStopping(patience=10, restore_best_weights=True),
                ReduceLROnPlateau(factor=0.5, patience=5)
            ],
            verbose=0
        )
        
        lstm_pred = lstm_model.predict(X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])).flatten()
        
        models['lstm'] = lstm_model
        performances['lstm'] = ModelPerformance(
            model_name='LSTM',
            mse=mean_squared_error(y_test, lstm_pred),
            mae=mean_absolute_error(y_test, lstm_pred),
            r2=r2_score(y_test, lstm_pred),
            accuracy=self._calculate_direction_accuracy(y_test, lstm_pred),
            last_trained=datetime.now(),
            training_samples=len(X_train)
        )
        
        # –°–æ–∑–¥–∞—ë–º –∞–Ω—Å–∞–º–±–ª—å
        ensemble_model = VotingRegressor([
            ('rf', rf_model),
            ('gb', gb_model),
            ('nn', nn_model)
        ])
        ensemble_model.fit(X_train_scaled, y_train)
        ensemble_pred = ensemble_model.predict(X_test_scaled)
        
        models['ensemble'] = ensemble_model
        performances['ensemble'] = ModelPerformance(
            model_name='Ensemble',
            mse=mean_squared_error(y_test, ensemble_pred),
            mae=mean_absolute_error(y_test, ensemble_pred),
            r2=r2_score(y_test, ensemble_pred),
            accuracy=self._calculate_direction_accuracy(y_test, ensemble_pred),
            last_trained=datetime.now(),
            training_samples=len(X_train)
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        self.price_prediction_models[symbol] = models
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.stats['models_trained'] += len(models)
        self.stats['last_training'] = datetime.now()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        best_model_name = max(performances.keys(), key=lambda x: performances[x].accuracy)
        self.stats['best_model'] = best_model_name
        
        logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name} (—Ç–æ—á–Ω–æ—Å—Ç—å: {performances[best_model_name].accuracy:.2%})")
        
        return {
            'models': models,
            'performances': performances,
            'best_model': best_model_name,
            'feature_names': features.columns.tolist()
        }
    
    def _create_lstm_model(self, input_dim: int) -> tf.keras.Model:
        """üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏"""
        model = Sequential([
            LSTM(50, return_sequences=True, input_shape=(1, input_dim)),
            Dropout(0.2),
            BatchNormalization(),
            LSTM(25, return_sequences=False),
            Dropout(0.2),
            BatchNormalization(),
            Dense(25, activation='relu'),
            Dropout(0.2),
            Dense(1, activation='linear')
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        return model
    
    def _calculate_direction_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """üìä –†–∞—Å—á—ë—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        true_direction = np.diff(y_true) > 0
        pred_direction = np.diff(y_pred) > 0
        
        if len(true_direction) == 0:
            return 0.0
        
        return np.mean(true_direction == pred_direction)
    
    def predict_price(self, symbol: str, features: pd.DataFrame) -> PredictionResult:
        """üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω—ã"""
        if symbol not in self.price_prediction_models:
            logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –¥–ª—è {symbol} –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return None
        
        models = self.price_prediction_models[symbol]
        scaler = self.scalers.get(f'{symbol}_price')
        
        if scaler is None:
            logger.error(f"‚ùå Scaler –¥–ª—è {symbol} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        X = features.iloc[-1:].values  # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞
        X_scaled = scaler.transform(X)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        predictions = {}
        
        for model_name, model in models.items():
            if model_name == 'lstm':
                pred = model.predict(X_scaled.reshape(1, 1, X_scaled.shape[1])).flatten()[0]
            else:
                pred = model.predict(X_scaled)[0]
            
            predictions[model_name] = pred
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        weights = self.settings['ensemble_weights']
        weighted_prediction = sum(predictions[model] * weights.get(model, 0.25) for model in predictions)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
        predictions_array = np.array(list(predictions.values()))
        confidence = 1.0 - (np.std(predictions_array) / np.mean(predictions_array)) if np.mean(predictions_array) != 0 else 0.5
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞
        current_price = features['price_change_1h'].iloc[-1] if 'price_change_1h' in features.columns else 0
        trend_direction = 'bullish' if weighted_prediction > current_price * 1.01 else 'bearish' if weighted_prediction < current_price * 0.99 else 'sideways'
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.stats['total_predictions'] += 1
        
        return PredictionResult(
            symbol=symbol,
            predicted_price=weighted_prediction,
            confidence=min(max(confidence, 0.0), 1.0),
            trend_direction=trend_direction,
            time_horizon='short',
            model_used='ensemble',
            features_importance=self._get_feature_importance(symbol, features)
        )
    
    def _get_feature_importance(self, symbol: str, features: pd.DataFrame) -> Dict[str, float]:
        """üìä –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if symbol not in self.price_prediction_models:
            return {}
        
        models = self.price_prediction_models[symbol]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Random Forest –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if 'random_forest' in models:
            rf_model = models['random_forest']
            importance = rf_model.feature_importances_
            feature_names = features.columns.tolist()
            
            return dict(zip(feature_names, importance))
        
        return {}
    
    def optimize_strategy_parameters(self, historical_trades: List[Dict]) -> Dict[str, Any]:
        """‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        logger.info("‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏")
        
        if len(historical_trades) < self.settings['min_training_samples']:
            logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {len(historical_trades)} < {self.settings['min_training_samples']}")
            return {}
        
        # –°–æ–∑–¥–∞—ë–º DataFrame –∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–¥–µ–ª–æ–∫
        df = pd.DataFrame(historical_trades)
        
        # –°–æ–∑–¥–∞—ë–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        features = self._create_strategy_features(df)
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è - –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å
        target = df['pnl_percent'].values
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.model_selection import cross_val_score
        
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        scores = cross_val_score(model, features, target, cv=5, scoring='neg_mean_squared_error')
        
        model.fit(features, target)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        feature_importance = dict(zip(features.columns, model.feature_importances_))
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        recommendations = {}
        
        for param, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):
            if importance > 0.1:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                recommendations[param] = {
                    'importance': importance,
                    'current_value': df[param].mean() if param in df.columns else 'unknown',
                    'suggestion': 'optimize' if importance > 0.2 else 'monitor'
                }
        
        logger.info(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ù–∞–π–¥–µ–Ω–æ {len(recommendations)} –∑–Ω–∞—á–∏–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        return {
            'model_score': scores.mean(),
            'feature_importance': feature_importance,
            'recommendations': recommendations,
            'optimization_date': datetime.now().isoformat()
        }
    
    def _create_strategy_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """üîß –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        features = pd.DataFrame()
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        if 'confidence' in df.columns:
            features['avg_confidence'] = df['confidence']
            features['confidence_std'] = df['confidence'].rolling(10).std()
        
        if 'volume_ratio' in df.columns:
            features['avg_volume_ratio'] = df['volume_ratio']
            features['volume_ratio_std'] = df['volume_ratio'].rolling(10).std()
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            features['hour_of_day'] = df['timestamp'].dt.hour
            features['day_of_week'] = df['timestamp'].dt.dayofweek
            features['is_weekend'] = (features['day_of_week'] >= 5).astype(int)
        
        # –†—ã–Ω–æ—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
        if 'market_condition' in df.columns:
            features['market_bullish'] = (df['market_condition'] == 'bullish').astype(int)
            features['market_bearish'] = (df['market_condition'] == 'bearish').astype(int)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–æ—Ä–≥–æ–≤
        features['win_rate'] = (df['pnl_percent'] > 0).rolling(20).mean()
        features['avg_pnl'] = df['pnl_percent'].rolling(20).mean()
        features['pnl_volatility'] = df['pnl_percent'].rolling(20).std()
        
        return features.fillna(0)
    
    def get_ml_statistics(self) -> Dict:
        """üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ML —Å–∏—Å—Ç–µ–º—ã"""
        return {
            'total_predictions': self.stats['total_predictions'],
            'models_trained': self.stats['models_trained'],
            'last_training': self.stats['last_training'].isoformat() if self.stats['last_training'] else None,
            'best_model': self.stats['best_model'],
            'avg_accuracy': self.stats['avg_accuracy'],
            'symbols_with_models': list(self.price_prediction_models.keys()),
            'settings': self.settings
        }
    
    def save_models(self, filepath: str = "ml_models.pkl"):
        """üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"""
        model_data = {
            'price_prediction_models': self.price_prediction_models,
            'scalers': self.scalers,
            'stats': self.stats,
            'settings': self.settings
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"üíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")
    
    def load_models(self, filepath: str = "ml_models.pkl"):
        """üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π"""
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.price_prediction_models = model_data.get('price_prediction_models', {})
            self.scalers = model_data.get('scalers', {})
            self.stats = model_data.get('stats', self.stats)
            self.settings = model_data.get('settings', self.settings)
            
            logger.info(f"üìÇ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {filepath}")
            
        except FileNotFoundError:
            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    ml_system = AdvancedMLSystem()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='1H')
    test_data = pd.DataFrame({
        'close': np.random.randn(len(dates)).cumsum() + 100,
        'volume': np.random.randint(1000, 10000, len(dates))
    }, index=dates)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    test_indicators = {
        'rsi': np.random.uniform(20, 80, len(dates)),
        'bb_upper': test_data['close'] * 1.02,
        'bb_lower': test_data['close'] * 0.98,
        'ema_21': test_data['close'].rolling(21).mean(),
        'ema_50': test_data['close'].rolling(50).mean(),
        'macd': np.random.randn(len(dates)),
        'macd_signal': np.random.randn(len(dates))
    }
    
    print("üß† –¢–ï–°–¢ –ü–†–û–î–í–ò–ù–£–¢–û–ô –°–ò–°–¢–ï–ú–´ ML")
    print("=" * 40)
    
    # –°–æ–∑–¥–∞—ë–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    features = ml_system.create_features(test_data, test_indicators)
    targets = ml_system.create_targets(test_data, [1, 3, 6])
    
    print(f"–°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features.shape[1]}")
    print(f"–°–æ–∑–¥–∞–Ω–æ —Ü–µ–ª–µ–π: {len(targets)}")
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    if len(features) > 1000:
        result = ml_system.train_price_prediction_model('BTCUSDT', features, targets)
        print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {result['best_model']}")
        
        # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = ml_system.predict_price('BTCUSDT', features)
        if prediction:
            print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞: {prediction.predicted_price:.2f}")
            print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {prediction.confidence:.2%}")
            print(f"–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞: {prediction.trend_direction}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = ml_system.get_ml_statistics()
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"–û–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {stats['models_trained']}")
    print(f"–°–∏–º–≤–æ–ª–æ–≤ —Å –º–æ–¥–µ–ª—è–º–∏: {len(stats['symbols_with_models'])}")
    
    print("‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω!")
