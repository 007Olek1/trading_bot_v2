#!/usr/bin/env python3
"""
üéì ML Trainer V2.0 - Enhanced with Early Stopping & AUC
–û–±—É—á–µ–Ω–∏–µ DistilBERT —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏

–£–õ–£–ß–®–ï–ù–ò–Ø:
- ‚úÖ Early Stopping (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è)
- ‚úÖ AUC –º–µ—Ç—Ä–∏–∫–∞ (–≤–∞–∂–Ω–µ–µ Accuracy –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞!)
- ‚úÖ Class Balancing (—É—á—ë—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤)
- ‚úÖ Learning Rate Scheduling
- ‚úÖ Model Checkpointing
- ‚úÖ Validation –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
- ‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –º–µ—Ç—Ä–∏–∫ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
"""

import asyncio
import logging
from typing import List, Dict, Tuple, Any
import json
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
from pathlib import Path
from dataclasses import dataclass
import pickle

logger = logging.getLogger(__name__)

logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s][%(levelname)s] %(message)s"
)

try:
    from transformers import (
        DistilBertTokenizer,
        DistilBertForSequenceClassification,
        Trainer,
        TrainingArguments,
        EarlyStoppingCallback
    )
    from datasets import Dataset
    import torch
    from sklearn.metrics import (
        accuracy_score,
        precision_recall_fscore_support,
        roc_auc_score,
        confusion_matrix,
        classification_report
    )
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.error("‚ùå transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! pip install transformers datasets torch scikit-learn")
    # Mock –∫–ª–∞—Å—Å—ã –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫ –∏–º–ø–æ—Ä—Ç–∞
    Dataset = object
    torch = None


@dataclass
class TrainingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    model_name: str = "distilbert-base-uncased"
    output_dir: str = "./models/distilbert_market_v2"
    
    # –û–±—É—á–µ–Ω–∏–µ
    num_epochs: int = 10
    batch_size: int = 16
    learning_rate: float = 2e-5
    warmup_steps: int = 500
    weight_decay: float = 0.01
    
    # Early Stopping
    early_stopping_patience: int = 3
    early_stopping_threshold: float = 0.001
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    eval_strategy: str = "epoch"
    save_strategy: str = "epoch"
    load_best_model: bool = True
    metric_for_best_model: str = "eval_auc"  # ‚Üê –ö–õ–Æ–ß–ï–í–û–ï!
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    fp16: bool = True  # Mixed precision (GPU)
    gradient_accumulation_steps: int = 1
    max_grad_norm: float = 1.0
    
    # –î–∞–Ω–Ω—ã–µ
    max_length: int = 128
    test_size: float = 0.2
    val_size: float = 0.1


class EnhancedDatasetBuilder:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–±–æ—Ä—â–∏–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤
    """
    
    def __init__(self):
        self.data = []
        logger.info("üì¶ Enhanced Dataset Builder –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def build_from_historical_data(
        self,
        exchange_manager,
        symbols: List[str],
        days: int = 60,
        balance_classes: bool = True
    ) -> pd.DataFrame:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤
        """
        from bot_v2_nlp_analyzer_v2 import OptimizedFeatureExtractor
        
        extractor = OptimizedFeatureExtractor()
        all_data = []
        
        logger.info(f"üìä –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ {len(symbols)} —Å–∏–º–≤–æ–ª–∞–º –∑–∞ {days} –¥–Ω–µ–π...")
        
        for symbol in symbols:
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–µ—á–∏
                candles = await exchange_manager.fetch_ohlcv(
                    symbol,
                    timeframe='1h',
                    limit=days * 24
                )
                
                if len(candles) < 50:
                    continue
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
                df = pd.DataFrame(candles)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                features = extractor._calculate_features_vectorized(df)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
                features['text'] = extractor._features_to_text_vectorized(features)
                
                # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (–±—É–¥—É—â–µ–µ –¥–≤–∏–∂–µ–Ω–∏–µ)
                # –°–º–æ—Ç—Ä–∏–º –Ω–∞ 5 —Å–≤–µ—á–µ–π –≤–ø–µ—Ä—ë–¥
                future_price = df['close'].shift(-5)
                current_price = df['close']
                price_change = ((future_price - current_price) / current_price) * 100
                
                # –ö–ª–∞—Å—Å—ã: 0 = –ü–ê–î–ï–ù–ò–ï, 1 = –ë–û–ö–û–í–ò–ö, 2 = –†–û–°–¢
                features['label'] = 1  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –±–æ–∫–æ–≤–∏–∫
                features.loc[price_change > 2, 'label'] = 2  # –†–æ—Å—Ç
                features.loc[price_change < -2, 'label'] = 0  # –ü–∞–¥–µ–Ω–∏–µ
                
                features['symbol'] = symbol
                features['price_change'] = price_change
                
                # –£–±–∏—Ä–∞–µ–º NaN
                features = features.dropna(subset=['text', 'label'])
                
                all_data.append(features[['text', 'label', 'symbol', 'price_change']])
                
                logger.info(f"‚úÖ {symbol}: {len(features)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ {symbol}: {e}")
                continue
        
        if not all_data:
            logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö!")
            return pd.DataFrame()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º
        df = pd.concat(all_data, ignore_index=True)
        
        # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
        if balance_classes:
            df = self._balance_classes(df)
        
        logger.info(f"üì¶ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        label_dist = df['label'].value_counts().to_dict()
        logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {label_dist}")
        
        return df
    
    def _balance_classes(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å—ã (undersampling/oversampling)"""
        
        # –°—á–∏—Ç–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        class_counts = df['label'].value_counts()
        min_count = class_counts.min()
        
        logger.info(f"‚öñÔ∏è –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–æ {min_count} –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–∞–∂–¥–æ–≥–æ")
        
        # Undersample –∫ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –∫–ª–∞—Å—Å—É
        balanced_dfs = []
        for label in df['label'].unique():
            class_df = df[df['label'] == label]
            sampled = class_df.sample(n=min(min_count, len(class_df)), random_state=42)
            balanced_dfs.append(sampled)
        
        balanced = pd.concat(balanced_dfs, ignore_index=True)
        balanced = balanced.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle
        
        logger.info(f"‚úÖ –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–æ: {len(balanced)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        return balanced
    
    def save_dataset(self, df: pd.DataFrame, filename: str = 'market_dataset_v2.csv'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç"""
        df.to_csv(filename, index=False)
        logger.info(f"üíæ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {filename}")
    
    def load_dataset(self, filename: str = 'market_dataset_v2.csv') -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç"""
        df = pd.read_csv(filename)
        logger.info(f"üìÇ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        return df


class EnhancedDistilBERTTrainer:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä —Å Early Stopping –∏ AUC
    """
    
    def __init__(self, config: TrainingConfig = None):
        self.config = config or TrainingConfig()
        self.model = None
        self.tokenizer = None
        self.trainer = None
        self.metrics_history = []
        
        if not TRANSFORMERS_AVAILABLE:
            logger.error("‚ùå transformers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
            return
        
        self.tokenizer = DistilBertTokenizer.from_pretrained(self.config.model_name)
        logger.info(f"ü§ñ Enhanced Trainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.config.model_name}")
    
    def prepare_dataset(self, df: pd.DataFrame) -> Dict[str, Dataset]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å train/val/test split
        """
        from sklearn.model_selection import train_test_split
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏
        texts = df['text'].tolist()
        labels = df['label'].astype(int).tolist()
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        encodings = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=self.config.max_length
        )
        
        # Train/Temp split
        X_train, X_temp, y_train, y_temp = train_test_split(
            encodings['input_ids'],
            labels,
            test_size=self.config.test_size + self.config.val_size,
            random_state=42,
            stratify=labels
        )
        
        # Val/Test split
        val_test_ratio = self.config.val_size / (self.config.test_size + self.config.val_size)
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp,
            y_temp,
            test_size=1 - val_test_ratio,
            random_state=42,
            stratify=y_temp
        )
        
        # –°–æ–∑–¥–∞—ë–º datasets
        def create_dataset(input_ids, labels):
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ —á–µ—Ä–µ–∑ tokenizer
            texts_subset = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)
            encodings_subset = self.tokenizer(
                texts_subset,
                truncation=True,
                padding=True,
                max_length=self.config.max_length
            )
            
            return Dataset.from_dict({
                'input_ids': encodings_subset['input_ids'],
                'attention_mask': encodings_subset['attention_mask'],
                'labels': labels
            })
        
        datasets = {
            'train': create_dataset(X_train, y_train),
            'validation': create_dataset(X_val, y_val),
            'test': create_dataset(X_test, y_test)
        }
        
        logger.info(f"üìö Datasets –≥–æ—Ç–æ–≤—ã:")
        logger.info(f"   Train: {len(datasets['train'])}")
        logger.info(f"   Validation: {len(datasets['validation'])}")
        logger.info(f"   Test: {len(datasets['test'])}")
        
        return datasets
    
    def compute_metrics(self, eval_pred):
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤–∫–ª—é—á–∞—è AUC
        """
        predictions, labels = eval_pred
        
        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        probs = torch.softmax(torch.from_numpy(predictions), dim=1).numpy()
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        preds = np.argmax(predictions, axis=1)
        
        # Accuracy
        accuracy = accuracy_score(labels, preds)
        
        # Precision, Recall, F1
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels, preds, average='weighted', zero_division=0
        )
        
        # AUC (one-vs-rest –¥–ª—è multiclass)
        try:
            auc = roc_auc_score(labels, probs, multi_class='ovr', average='weighted')
        except ValueError:
            auc = 0.5
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc  # ‚Üê –í–ê–ñ–ù–ï–ô–®–ê–Ø –ú–ï–¢–†–ò–ö–ê!
        }
    
    def train(self, datasets: Dict[str, Dataset]):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Å Early Stopping
        """
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model = DistilBertForSequenceClassification.from_pretrained(
            self.config.model_name,
            num_labels=3,  # –ü–ê–î–ï–ù–ò–ï, –ë–û–ö–û–í–ò–ö, –†–û–°–¢
            problem_type="single_label_classification"
        )
        
        # –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        import logging as transformers_logging
        transformers_logging.getLogger("transformers.modeling_utils").setLevel(transformers_logging.ERROR)
        
        # –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_epochs,
            per_device_train_batch_size=self.config.batch_size,
            per_device_eval_batch_size=self.config.batch_size * 2,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            weight_decay=self.config.weight_decay,
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
            fp16=self.config.fp16 and torch.cuda.is_available(),
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            max_grad_norm=self.config.max_grad_norm,
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            eval_strategy=self.config.eval_strategy,
            save_strategy=self.config.save_strategy,
            load_best_model_at_end=self.config.load_best_model,
            metric_for_best_model=self.config.metric_for_best_model,
            greater_is_better=True,  # AUC –±–æ–ª—å—à–µ = –ª—É—á—à–µ
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            logging_dir=f'{self.config.output_dir}/logs',
            logging_steps=50,
            report_to="none",
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            save_total_limit=2,  # –¢–æ–ª—å–∫–æ 2 –ª—É—á—à–∏—Ö checkpoint
        )
        
        # Trainer
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=datasets['train'],
            eval_dataset=datasets['validation'],
            compute_metrics=self.compute_metrics,
            callbacks=[
                EarlyStoppingCallback(
                    early_stopping_patience=self.config.early_stopping_patience,
                    early_stopping_threshold=self.config.early_stopping_threshold
                )
            ]
        )
        
        logger.info("üéì –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å Early Stopping...")
        logger.info(f"   Epochs: {self.config.num_epochs}")
        logger.info(f"   Batch size: {self.config.batch_size}")
        logger.info(f"   Learning rate: {self.config.learning_rate}")
        logger.info(f"   Early stopping patience: {self.config.early_stopping_patience}")
        logger.info(f"   Metric: {self.config.metric_for_best_model}")
        
        # –û–±—É—á–µ–Ω–∏–µ
        train_result = self.trainer.train()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        self.trainer.save_model(self.config.output_dir)
        self.tokenizer.save_pretrained(self.config.output_dir)
        
        logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.config.output_dir}")
        
        return train_result
    
    def evaluate(self, datasets: Dict[str, Dataset], split: str = 'test') -> Dict[str, float]:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
        """
        logger.info(f"üìä –û—Ü–µ–Ω–∫–∞ –Ω–∞ {split} –≤—ã–±–æ—Ä–∫–µ...")
        
        results = self.trainer.evaluate(datasets[split])
        
        logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ {split}:")
        for metric, value in results.items():
            logger.info(f"   {metric}: {value:.4f}")
        
        return results
    
    def predict(self, texts: List[str]) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Å—ã –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        
        Returns:
            (predicted_classes, probabilities)
        """
        if not self.model:
            logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            return np.array([]), np.array([])
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        encodings = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=self.config.max_length,
            return_tensors='pt'
        )
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(**encodings)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=1).numpy()
            preds = np.argmax(probs, axis=1)
        
        return preds, probs
    
    def save_metrics(self, metrics: Dict, filename: str = 'training_metrics.json'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏"""
        metrics['timestamp'] = datetime.now().isoformat()
        
        with open(f"{self.config.output_dir}/{filename}", 'w') as f:
            json.dump(metrics, f, indent=2)
        
        logger.info(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filename}")
    
    def load_model(self, model_dir: str = None):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        model_dir = model_dir or self.config.output_dir
        
        try:
            self.model = DistilBertForSequenceClassification.from_pretrained(model_dir)
            self.tokenizer = DistilBertTokenizer.from_pretrained(model_dir)
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_dir}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")


async def main():
    """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
    
    if not TRANSFORMERS_AVAILABLE:
        print("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers datasets torch scikit-learn")
        return
    
    print("\n" + "="*70)
    print("üéì ML TRAINER V2.0 - ENHANCED WITH EARLY STOPPING & AUC")
    print("="*70)
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = TrainingConfig(
        num_epochs=5,
        batch_size=8,
        early_stopping_patience=2,
        metric_for_best_model="eval_auc"
    )
    
    # 1. –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –®–ê–ì 1: –°–±–æ—Ä –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    from bot_v2_exchange import ExchangeManager
    
    exchange = ExchangeManager()
    await exchange.connect()
    
    builder = EnhancedDatasetBuilder()
    
    # –¢–æ–ø —Å–∏–º–≤–æ–ª—ã
    symbols = [
        'BTC/USDT:USDT', 'ETH/USDT:USDT', 'SOL/USDT:USDT',
        'BNB/USDT:USDT', 'XRP/USDT:USDT'
    ]
    
    df = await builder.build_from_historical_data(exchange, symbols, days=14, balance_classes=True)
    
    await exchange.disconnect()
    
    if df.empty or len(df) < 100:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö!")
        return
    
    builder.save_dataset(df)
    
    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüìö –®–ê–ì 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ datasets...")
    
    trainer = EnhancedDistilBERTTrainer(config)
    datasets = trainer.prepare_dataset(df)
    
    # 3. –û–±—É—á–µ–Ω–∏–µ
    print("\nüéì –®–ê–ì 3: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    train_result = trainer.train(datasets)
    
    # 4. –û—Ü–µ–Ω–∫–∞
    print("\nüìä –®–ê–ì 4: –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...")
    
    test_metrics = trainer.evaluate(datasets, split='test')
    trainer.save_metrics(test_metrics)
    
    # 5. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print("\nüß™ –®–ê–ì 5: –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
    
    test_texts = [
        "price rising strongly volume surging near resistance",
        "price falling sharply volume declining oversold",
        "price consolidating sideways movement volume stable"
    ]
    
    preds, probs = trainer.predict(test_texts)
    
    class_names = ['–ü–ê–î–ï–ù–ò–ï', '–ë–û–ö–û–í–ò–ö', '–†–û–°–¢']
    
    for i, text in enumerate(test_texts):
        pred_class = class_names[preds[i]]
        confidence = probs[i][preds[i]]
        print(f"\n   '{text[:50]}...'")
        print(f"   ‚Üí {pred_class} ({confidence:.1%} confidence)")
        print(f"   Probabilities: {dict(zip(class_names, probs[i]))}")
    
    print("\n" + "="*70)
    print("‚úÖ –í–°–Å –ì–û–¢–û–í–û!")
    print(f"üìä Test AUC: {test_metrics.get('eval_auc', 0):.3f}")
    print(f"üìä Test Accuracy: {test_metrics.get('eval_accuracy', 0):.3f}")
    print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config.output_dir}")
    print("="*70 + "\n")


if __name__ == "__main__":
    asyncio.run(main())

