#!/usr/bin/env python3
"""
üöÄ NLP Market Analyzer V2.0 - OPTIMIZED
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —Ç—Ä–æ–∏—á–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π

–£–õ–£–ß–®–ï–ù–ò–Ø:
- ‚úÖ –¢—Ä–æ–∏—á–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (-1/0/+1) –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —à—É–º–∞
- ‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (10-100x –±—ã—Å—Ç—Ä–µ–µ)
- ‚úÖ Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Å–∏–º–≤–æ–ª–æ–≤
- ‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (HL range, momentum)
- ‚úÖ –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏/—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
"""

import asyncio
import logging
from typing import Dict, List, Any, Tuple
import numpy as np
import pandas as pd
from datetime import datetime
from dataclasses import dataclass

logger = logging.getLogger(__name__)

try:
    from transformers import pipeline
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("‚ö†Ô∏è transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è rule-based –ø–æ–¥—Ö–æ–¥.")


@dataclass
class FeatureConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    short_window: int = 3      # –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–π —Ç—Ä–µ–Ω–¥
    medium_window: int = 7     # –°—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω—ã–π —Ç—Ä–µ–Ω–¥
    long_window: int = 14      # –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π —Ç—Ä–µ–Ω–¥
    ternary_threshold: float = 0.001  # 0.1% –ø–æ—Ä–æ–≥ –¥–ª—è —Ç—Ä–æ–∏—á–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
    volume_surge_threshold: float = 1.5
    volume_increase_threshold: float = 1.2
    volume_decline_threshold: float = 0.7
    volatility_high_threshold: float = 0.03
    volatility_low_threshold: float = 0.01
    momentum_strong_threshold: float = 0.05


class OptimizedFeatureExtractor:
    """
    –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é —á–µ—Ä–µ–∑ NumPy/Pandas –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
    """
    
    def __init__(self, config: FeatureConfig = None):
        self.config = config or FeatureConfig()
        logger.info("üöÄ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _ternary_encode(self, series: pd.Series) -> pd.Series:
        """
        –¢—Ä–æ–∏—á–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: -1 (–ø–∞–¥–µ–Ω–∏–µ), 0 (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π), +1 (—Ä–æ—Å—Ç)
        
        –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –º–∏–∫—Ä–æ-–¥–≤–∏–∂–µ–Ω–∏—è < 0.1% –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —à—É–º–∞
        """
        changes = series.pct_change()
        threshold = self.config.ternary_threshold
        
        result = pd.Series(0, index=series.index, dtype=np.int8)
        result[changes > threshold] = 1
        result[changes < -threshold] = -1
        
        return result
    
    def _calculate_features_vectorized(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–´–ô —Ä–∞—Å—á—ë—Ç –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Pandas –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤–º–µ—Å—Ç–æ —Ü–∏–∫–ª–æ–≤ ‚Üí –í 10-100 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ!
        """
        features = pd.DataFrame(index=df.index)
        
        # 1. –¢–†–û–ò–ß–ù–û–ï –ö–û–î–ò–†–û–í–ê–ù–ò–ï
        close_ternary = self._ternary_encode(df['close'])
        features['close_ternary'] = close_ternary
        
        # 2. –¢–†–ï–ù–î–´ (—Å—É–º–º–∞ —Ç—Ä–æ–∏—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)
        features['short_trend'] = close_ternary.rolling(
            window=self.config.short_window
        ).sum()
        
        features['medium_trend'] = close_ternary.rolling(
            window=self.config.medium_window
        ).sum()
        
        # 3. –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–¨ (High-Low Range)
        features['hl_range'] = (
            (df['high'] - df['low']) / df['close']
        ).rolling(window=self.config.short_window).mean()
        
        # 4. VOLUME MOMENTUM
        avg_volume = df['volume'].rolling(window=self.config.long_window).mean()
        features['volume_momentum'] = df['volume'] / (avg_volume + 1e-9)
        
        # 5. PRICE MOMENTUM
        features['price_momentum'] = df['close'].pct_change(self.config.short_window)
        
        # 6. NEAR RESISTANCE/SUPPORT (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        max_high = df['high'].rolling(window=self.config.long_window).max()
        min_low = df['low'].rolling(window=self.config.long_window).min()
        
        features['near_resistance'] = (df['close'] / max_high) > 0.98
        features['near_support'] = (df['close'] / min_low) < 1.02
        
        # 7. TARGET (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML)
        features['target'] = (df['close'].shift(-1) > df['close']).astype(int)
        
        return features
    
    def _features_to_text_vectorized(self, features: pd.DataFrame) -> pd.Series:
        """
        –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç np.select –≤–º–µ—Å—Ç–æ —Ü–∏–∫–ª–æ–≤ ‚Üí –í 10-100 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ!
        """
        text_parts = []
        
        # === 1. SHORT TREND (price movement) ===
        conditions = [
            features['short_trend'] >= 2,
            features['short_trend'] >= 1,
            features['short_trend'] <= -2,
            features['short_trend'] <= -1
        ]
        choices = [
            "price rising strongly",
            "price rising",
            "price falling sharply",
            "price falling"
        ]
        text_parts.append(pd.Series(
            np.select(conditions, choices, default="price consolidating"),
            index=features.index
        ))
        
        # === 2. MEDIUM TREND (established trends) ===
        conditions = [
            features['medium_trend'] >= 4,
            features['medium_trend'] >= 2,
            features['medium_trend'] <= -4,
            features['medium_trend'] <= -2
        ]
        choices = [
            "strong uptrend",
            "uptrend established",
            "strong downtrend",
            "downtrend established"
        ]
        text_parts.append(pd.Series(
            np.select(conditions, choices, default="sideways movement"),
            index=features.index
        ))
        
        # === 3. VOLATILITY (HL range) ===
        conditions = [
            features['hl_range'] > self.config.volatility_high_threshold,
            features['hl_range'] < self.config.volatility_low_threshold
        ]
        choices = ["high volatility", "low volatility"]
        text_parts.append(pd.Series(
            np.select(conditions, choices, default="normal volatility"),
            index=features.index
        ))
        
        # === 4. VOLUME ===
        conditions = [
            features['volume_momentum'] > self.config.volume_surge_threshold,
            features['volume_momentum'] > self.config.volume_increase_threshold,
            features['volume_momentum'] < self.config.volume_decline_threshold
        ]
        choices = ["volume surging", "volume increasing", "volume declining"]
        text_parts.append(pd.Series(
            np.select(conditions, choices, default="volume stable"),
            index=features.index
        ))
        
        # === 5. MOMENTUM ===
        conditions = [
            features['price_momentum'] > self.config.momentum_strong_threshold,
            features['price_momentum'] < -self.config.momentum_strong_threshold
        ]
        choices = ["strong bullish momentum", "strong bearish momentum"]
        text_parts.append(pd.Series(
            np.select(conditions, choices, default=""),
            index=features.index
        ))
        
        # === 6. SUPPORT/RESISTANCE ===
        text_parts.append(
            features['near_resistance'].map({True: "near resistance", False: ""})
        )
        text_parts.append(
            features['near_support'].map({True: "near support", False: ""})
        )
        
        # === –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï ===
        combined = pd.concat(text_parts, axis=1)
        return combined.apply(
            lambda row: ' '.join([str(x) for x in row if x and str(x).strip()]).strip(),
            axis=1
        )
    
    def process_single_symbol(
        self,
        candles: List[Dict],
        symbol: str = None
    ) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        
        Returns:
            {
                'features': DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏,
                'text': str –ø–æ—Å–ª–µ–¥–Ω–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ,
                'ternary': dict —Å —Ç—Ä–æ–∏—á–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            }
        """
        if len(candles) < self.config.long_window + 1:
            return {
                'features': None,
                'text': 'insufficient data',
                'ternary': {}
            }
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
        df = pd.DataFrame(candles)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = self._calculate_features_vectorized(df)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        features['text'] = self._features_to_text_vectorized(features)
        
        # –£–±–∏—Ä–∞–µ–º NaN
        features = features.dropna(subset=['text'])
        
        if len(features) == 0:
            return {
                'features': None,
                'text': 'no valid features',
                'ternary': {}
            }
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        last_row = features.iloc[-1]
        
        return {
            'features': features,
            'text': last_row['text'],
            'ternary': {
                'short_trend': int(last_row['short_trend']),
                'medium_trend': int(last_row['medium_trend']),
                'volatility': float(last_row['hl_range']),
                'volume_momentum': float(last_row['volume_momentum']),
                'price_momentum': float(last_row['price_momentum']),
                'near_resistance': bool(last_row['near_resistance']),
                'near_support': bool(last_row['near_support'])
            }
        }
    
    def process_batch(
        self,
        symbols_data: List[Dict[str, Any]]
    ) -> pd.DataFrame:
        """
        BATCH –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Å–∏–º–≤–æ–ª–æ–≤
        
        Args:
            symbols_data: [
                {
                    'symbol': 'BTC/USDT',
                    'candles': [...],
                    'indicators': {...}
                },
                ...
            ]
        
        Returns:
            DataFrame —Å–æ –≤—Å–µ–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏ –∏ –∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        all_features = []
        
        for data in symbols_data:
            symbol = data['symbol']
            candles = data['candles']
            
            if len(candles) < self.config.long_window + 1:
                continue
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
            df = pd.DataFrame(candles)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self._calculate_features_vectorized(df)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–º–≤–æ–ª
            features['symbol'] = symbol
            
            all_features.append(features)
        
        if not all_features:
            return pd.DataFrame()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ
        combined_df = pd.concat(all_features, ignore_index=True)
        
        # –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –í–°–ï–• —Å–∏–º–≤–æ–ª–æ–≤ —Å—Ä–∞–∑—É!
        combined_df['text'] = self._features_to_text_vectorized(combined_df)
        
        # –£–±–∏—Ä–∞–µ–º NaN
        combined_df = combined_df.dropna(subset=['text'])
        
        logger.info(
            f"‚úÖ Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(symbols_data)} —Å–∏–º–≤–æ–ª–æ–≤, "
            f"{len(combined_df)} –∑–∞–ø–∏—Å–µ–π"
        )
        
        return combined_df


class NLPMarketAnalyzerV2:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π NLP –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    """
    
    def __init__(self):
        self.extractor = OptimizedFeatureExtractor()
        self.classifier = None
        
        logger.info("ü§ñ NLP Market Analyzer V2.0 –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        if TRANSFORMERS_AVAILABLE:
            self._load_classifier()
    
    def _load_classifier(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞"""
        try:
            self.classifier = pipeline(
                "zero-shot-classification",
                model="facebook/bart-large-mnli",
                device=0 if torch.cuda.is_available() else -1
            )
            logger.info("‚úÖ Zero-shot classifier –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: {e}")
            self.classifier = None
    
    def classify_market_state(
        self,
        description: str,
        ternary_features: Dict = None
    ) -> Tuple[str, float]:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä—ã–Ω–∫–∞
        
        Returns:
            (state, confidence) - "–†–û–°–¢"/"–ü–ê–î–ï–ù–ò–ï"/"–ë–û–ö–û–í–ò–ö" –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        """
        if self.classifier and TRANSFORMERS_AVAILABLE:
            return self._classify_with_model(description)
        else:
            return self._classify_rule_based(description, ternary_features)
    
    def _classify_with_model(self, description: str) -> Tuple[str, float]:
        """ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"""
        try:
            result = self.classifier(
                description,
                candidate_labels=["bullish trend", "bearish trend", "sideways movement"]
            )
            
            mapping = {
                "bullish trend": "–†–û–°–¢",
                "bearish trend": "–ü–ê–î–ï–ù–ò–ï",
                "sideways movement": "–ë–û–ö–û–í–ò–ö"
            }
            
            state = mapping.get(result['labels'][0], "–ë–û–ö–û–í–ò–ö")
            confidence = result['scores'][0]
            
            return state, confidence
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            return "–ë–û–ö–û–í–ò–ö", 0.5
    
    def _classify_rule_based(
        self,
        description: str,
        ternary_features: Dict = None
    ) -> Tuple[str, float]:
        """Rule-based –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å —Ç—Ä–æ–∏—á–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—Ä–æ–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        if ternary_features:
            short_trend = ternary_features.get('short_trend', 0)
            medium_trend = ternary_features.get('medium_trend', 0)
            
            # –°–∏–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –æ—Ç —Ç—Ä–æ–∏—á–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if medium_trend >= 4 or short_trend >= 2:
                return "–†–û–°–¢", 0.8
            elif medium_trend <= -4 or short_trend <= -2:
                return "–ü–ê–î–ï–ù–ò–ï", 0.8
            elif abs(medium_trend) >= 2:
                if medium_trend > 0:
                    return "–†–û–°–¢", 0.6
                else:
                    return "–ü–ê–î–ï–ù–ò–ï", 0.6
        
        # Fallback –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        desc_lower = description.lower()
        
        bullish_score = 0
        bearish_score = 0
        
        bullish_words = ['rising', 'surging', 'bullish', 'uptrend', 'strong']
        bearish_words = ['falling', 'declining', 'bearish', 'downtrend', 'weak']
        
        for word in bullish_words:
            if word in desc_lower:
                bullish_score += 1
        
        for word in bearish_words:
            if word in desc_lower:
                bearish_score += 1
        
        total = bullish_score + bearish_score
        
        if total == 0:
            return "–ë–û–ö–û–í–ò–ö", 0.5
        
        if bullish_score > bearish_score:
            confidence = 0.5 + (bullish_score / (total + 2)) * 0.3
            return "–†–û–°–¢", confidence
        elif bearish_score > bullish_score:
            confidence = 0.5 + (bearish_score / (total + 2)) * 0.3
            return "–ü–ê–î–ï–ù–ò–ï", confidence
        else:
            return "–ë–û–ö–û–í–ò–ö", 0.6
    
    async def analyze_symbol(
        self,
        symbol: str,
        candles: List[Dict],
        indicators: Dict[str, float] = None
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        """
        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
            result = self.extractor.process_single_symbol(candles, symbol)
            
            if result['text'] in ['insufficient data', 'no valid features']:
                return {
                    'symbol': symbol,
                    'description': result['text'],
                    'state': '–ë–û–ö–û–í–ò–ö',
                    'confidence': 0.0,
                    'ternary': {}
                }
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º
            state, confidence = self.classify_market_state(
                result['text'],
                result['ternary']
            )
            
            return {
                'symbol': symbol,
                'description': result['text'],
                'state': state,
                'confidence': confidence,
                'ternary': result['ternary'],
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {symbol}: {e}")
            return {
                'symbol': symbol,
                'description': 'error',
                'state': '–ë–û–ö–û–í–ò–ö',
                'confidence': 0.0,
                'ternary': {},
                'error': str(e)
            }
    
    async def analyze_batch(
        self,
        symbols_data: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        BATCH –∞–Ω–∞–ª–∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Å–∏–º–≤–æ–ª–æ–≤ (–ë–´–°–¢–†–û!)
        """
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º batch –∞–Ω–∞–ª–∏–∑ {len(symbols_data)} —Å–∏–º–≤–æ–ª–æ–≤...")
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        df = self.extractor.process_batch(symbols_data)
        
        if df.empty:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return []
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        latest = df.groupby('symbol').tail(1)
        
        results = []
        for _, row in latest.iterrows():
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
            state, confidence = self.classify_market_state(
                row['text'],
                {
                    'short_trend': int(row['short_trend']),
                    'medium_trend': int(row['medium_trend'])
                }
            )
            
            results.append({
                'symbol': row['symbol'],
                'description': row['text'],
                'state': state,
                'confidence': confidence,
                'ternary': {
                    'short_trend': int(row['short_trend']),
                    'medium_trend': int(row['medium_trend']),
                    'volatility': float(row['hl_range']),
                    'volume_momentum': float(row['volume_momentum']),
                    'near_resistance': bool(row['near_resistance']),
                    'near_support': bool(row['near_support'])
                },
                'timestamp': datetime.now().isoformat()
            })
        
        logger.info(f"‚úÖ Batch –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω: {len(results)} —Å–∏–º–≤–æ–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")
        
        return results


# Singleton
nlp_analyzer_v2 = NLPMarketAnalyzerV2()


async def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    import time
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    def generate_test_candles(trend='bullish', count=20):
        candles = []
        price = 100.0
        for i in range(count):
            if trend == 'bullish':
                change = np.random.uniform(0.5, 2.0)
            elif trend == 'bearish':
                change = np.random.uniform(-2.0, -0.5)
            else:
                change = np.random.uniform(-0.5, 0.5)
            
            price *= (1 + change / 100)
            candles.append({
                'open': price * 0.99,
                'high': price * 1.01,
                'low': price * 0.98,
                'close': price,
                'volume': np.random.uniform(800, 1200)
            })
        return candles
    
    print("\n" + "="*70)
    print("üöÄ –¢–ï–°–¢ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û NLP ANALYZER V2.0")
    print("="*70)
    
    # === –¢–ï–°–¢ 1: –û–¥–∏–Ω–æ—á–Ω—ã–π —Å–∏–º–≤–æ–ª ===
    print("\nüìä –¢–ï–°–¢ 1: –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞")
    print("-" * 70)
    
    candles = generate_test_candles('bullish', 20)
    
    start = time.time()
    result = await nlp_analyzer_v2.analyze_symbol('BTC/USDT', candles)
    elapsed = time.time() - start
    
    print(f"Symbol: {result['symbol']}")
    print(f"Description: {result['description']}")
    print(f"State: {result['state']} ({result['confidence']:.0%} confidence)")
    print(f"Ternary: {result['ternary']}")
    print(f"‚è±Ô∏è  –í—Ä–µ–º—è: {elapsed*1000:.1f}ms")
    
    # === –¢–ï–°–¢ 2: Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ ===
    print("\n" + "="*70)
    print("üöÄ –¢–ï–°–¢ 2: Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ 50 —Å–∏–º–≤–æ–ª–æ–≤")
    print("-" * 70)
    
    symbols_data = []
    for i in range(50):
        trend = np.random.choice(['bullish', 'bearish', 'sideways'])
        symbols_data.append({
            'symbol': f'SYM{i}/USDT',
            'candles': generate_test_candles(trend, 20)
        })
    
    start = time.time()
    results = await nlp_analyzer_v2.analyze_batch(symbols_data)
    elapsed = time.time() - start
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    states = {}
    for r in results:
        states[r['state']] = states.get(r['state'], 0) + 1
    
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"‚è±Ô∏è  –í—Ä–µ–º—è: {elapsed:.2f}s ({elapsed*1000/len(results):.1f}ms –Ω–∞ —Å–∏–º–≤–æ–ª)")
    print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {states}")
    print(f"üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {len(results)/elapsed:.1f} —Å–∏–º–≤–æ–ª–æ–≤/—Å–µ–∫")
    
    # –ü—Ä–∏–º–µ—Ä—ã
    print("\nüìù –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
    for result in results[:5]:
        print(f"  ‚Ä¢ {result['symbol']}: {result['state']} - {result['description'][:50]}...")
    
    print("\n" + "="*70)
    print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´!")
    print("="*70)


if __name__ == "__main__":
    asyncio.run(main())

